{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title Here\n",
    "\n",
    "---\n",
    "Please fill out your details:\n",
    "\n",
    "Name = Dias Kuatbekov\n",
    "\n",
    "Student Id = 20709092\n",
    "\n",
    "Course Type = Taught Masters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrity statement\n",
    "\n",
    "**If not using this Jupyter notebook please make sure this statement is included in your main.py file**\n",
    "\n",
    "I, Dias Kuatbekov, have read and understood the School's Academic Integrity Policy, as well as guidance relating to this module, and confirm that this submission complies with the policy. The content of this file is my own original work, with any significant material copied or adapted from other sources clearly indicated and attributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**This cell can be deleted when you are ready to submit but we'd leave it as a checklist for yourself until that time**\n",
    "\n",
    "## Project Template\n",
    "\n",
    "This project template can be used for the submission of your project. Further details including dates and details of assessment are included in the appropriate part of the course book. You should structure the notebook to include markdown and code cells. Documentation should be included in docstrings / comments in your code cells, but markdown cells can be used to record decisions on your thought process; explanations of datasets; discussion of the functionality created and how it will help the user etc \n",
    "\n",
    "We would prefer submissions using this Jupyter Notebook (*.ipynb). The Jupyter Notebook should not only provide code but be structured with Markdown cells:\n",
    "\n",
    "- Please briefly outline the scenario.\n",
    "- Give a description of the aim of the project and the problem it is solving\n",
    "- A brief overview in words of how the code works\n",
    "- A discussion of design decisions taken in structuring the code.\n",
    "- How you would develop / improve the code if given more time?\n",
    "\n",
    "However, we are aware that some code eg guis does not work as well in a Jupyter notebook. If this is the case:\n",
    "\n",
    "- You should provide a `main.py` file which runs your code\n",
    "- Include any tests in a separate `tests.py` file\n",
    "- Provide a pdf which covers the points above. As a guide we'd expect 300-500 words\n",
    "\n",
    "In both formats, additional code / modules can also be included in other python files in the same folder and then imported. Make sure you use relative imports in your code.\n",
    "\n",
    "**Please make every effort to structure your code in a logical fashion to assist us in understanding it**\n",
    "\n",
    "In addition to the Jupyter Notebook / code you should also submit:\n",
    "\n",
    "- a Conda environment file which enables your notebook to be run. Please ensure you test this on a Windows machine.\n",
    "- any output that cannot easily be embedded in the notebook (e.g an example data file or some visualisation, pdfs). These should be referred to explicitly.\n",
    "\n",
    "Your program should (as a rough guide): \n",
    "\n",
    "- analyse real data or perform a simulation\n",
    "- define at least two user functions (but typically more)\n",
    "- make use of appropriate specialist modules\n",
    "- comprise >~ 50 lines of actual code (excluding comments, imports and other ‘boilerplate’)\n",
    "- contain no more than 1000 lines in total (if you have written more, please isolate an individual element). The additional code can be imported from a .py file but will not be marked.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "*Please group all your imports together in the code cell below and provide web links in this cell to either a documentation page or a github repository. The accompanying conda environment file should install the necessary modules.\n",
    "\n",
    "Supplementary pieces of code can be included in .py files in the top level of your project. In this box you should name each .py file and state to what extent it is your own work. If it is the work of others or you have modified others work then you should state this clearly and provide a link to the source of the original code. The default assumption is that all code is your own work unless otherwise stated.* \n",
    "\n",
    "### Import documentation links and contribution statements\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from external libraries\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from tkinter import StringVar\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tkcalendar import DateEntry\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from scipy.interpolate import griddata, Rbf\n",
    "import numpy as np\n",
    "import mplcursors\n",
    "import matplotlib.dates as mdates\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.figure import Figure\n",
    "from geopandas import GeoSeries\n",
    "\n",
    "# Imports from supplementary modules / code included with the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of project\n",
    "\n",
    "These headings are a guide to help you cover the key points. Whilst the first heading should remain here ahead of your code it may be more appropriate to move the other headings around to suit your project, placing them near relevant code cells. The choice is yours.\n",
    "\n",
    "### Aim of project and the problem it is solving\n",
    "\n",
    "### Brief overview of how code works\n",
    "\n",
    "### Design decisions made in structuring of the code\n",
    "\n",
    "### How you would improve / develop the code if given more time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code\n",
    "\n",
    "I suggest breaking your code up into multiple cells. Grouping related code together and interspersing with markdown cells where appropriate to explain what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gdf = pd.read_csv(\"data/sensors_chp.csv\")\n",
    "combined_gdf['geometry'] = combined_gdf['geometry'].apply(wkt.loads)\n",
    "combined_gdf = gpd.GeoDataFrame(combined_gdf, geometry='geometry')\n",
    "combined_gdf.set_crs('EPSG:4326', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulation of the data constitutes a huge and important part of the project. Therefore, as the project evolved, it became evident that there is a need for dedicating a whole separate class responssible only for handing data operations. Doing so allowed to keep the code more modular and debugging a lot easier.\n",
    "In general, DataManipulator class is responsible for:\n",
    "- Filtering out date ranges in a dataframe\n",
    "- Calculating Statistical information\n",
    "- Calculating pollution for each district in the city of Almaty\n",
    "- Interpolation\n",
    "- Since I did not know any obvious workaround, it also stores the locations of power plants. Though power plant locations are used only when plotting, I find that it keeps the code structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManipulator:\n",
    "    \"\"\"\n",
    "    The class aims to handle all the data manipulation that one can encounter when working on the project\n",
    "\n",
    "    Attributes:\n",
    "        gdf (GeoDataFrame): dataframe with sensor locations, coal burnt in power plants, and P.M. 2.5 readings\n",
    "        almaty_boundaries: a geo.json file with info about Almaty\n",
    "        power_plant_locs (GeoDataFrame): dataframe with locations of power plants\n",
    "    \"\"\"\n",
    "    def __init__(self, gdf, almaty_boundaries):\n",
    "        self.gdf = gdf\n",
    "        self.almaty_boundaries = almaty_boundaries\n",
    "\n",
    "        if self.gdf.crs != self.almaty_boundaries.crs:\n",
    "            self.gdf = self.gdf.to_crs(self.almaty_boundaries.crs)\n",
    "\n",
    "        power_plant_locs = [\n",
    "            {'location': 'Power Plant 2', 'geometry': Point(77.0061645, 43.4224249)},\n",
    "            {'location': 'Power Plant 3', 'geometry': Point(76.9278271, 43.280907)}\n",
    "        ]\n",
    "\n",
    "        self.power_plants_gdf = gpd.GeoDataFrame(power_plant_locs, crs=gdf.crs)\n",
    "\n",
    "    \n",
    "    def filter_data(self, start_date: pd.Timestamp, end_date: pd.Timestamp)-> gpd.GeoDataFrame:\n",
    "        \"\"\"\n",
    "        Filters Data according to the passed date range and returns a gdf with correct crs\n",
    "\n",
    "        Args:\n",
    "            start_date (pd.TimeStamp): Filtering start date\n",
    "            end_date (pd.TimeStamp): Filtering end date\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: a gdf that has data only for filtered out dates\n",
    "        \"\"\"\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        return self.gdf[(self.gdf['date'] >= start_date_str) & (self.gdf['date'] <= end_date_str)]\n",
    "        \n",
    "\n",
    "    def get_statistics(self, gdf: gpd.GeoDataFrame):\n",
    "        \"\"\"\n",
    "        Computation of basic statistical information based on the provided dataframe\n",
    "\n",
    "        Args:\n",
    "            gdf (GeoDataFrame): gdf containing sensor readings\n",
    "\n",
    "        Returns:\n",
    "            tuple: min, max, avg, var and most polluted district\n",
    "        \"\"\"\n",
    "\n",
    "        if 'Reading' not in gdf.columns or gdf['Reading'].isnull().all(): # maybe catch error? !!!!!!!!!!!!!!!!!!!!!!\n",
    "            return None, None, None, None\n",
    "        \n",
    "        min = gdf['Reading'].min()\n",
    "        max = gdf['Reading'].max()\n",
    "        avg = gdf['Reading'].mean()\n",
    "        var = gdf['Reading'].var()\n",
    "\n",
    "        joined_gdf = gpd.sjoin(gdf, self.almaty_boundaries, how='inner', predicate='intersects') # this has to be reworked !!!!!!!!!\n",
    "        district_avg = joined_gdf.groupby('name_left')['Reading'].mean()\n",
    "\n",
    "        most_polluted_district = district_avg.idxmax() if not district_avg.empty else None\n",
    "\n",
    "        return min, max, avg, var, most_polluted_district\n",
    "    \n",
    "\n",
    "    def get_power_plant_data(self):\n",
    "        \"\"\"\n",
    "        Power plant data\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: gdf with power plant locations\n",
    "        \"\"\"\n",
    "        return self.power_plants_gdf\n",
    "    \n",
    "    \n",
    "    def calculate_pollution_by_district(self, gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        \"\"\"\n",
    "        Generates a GeoDataFrame with Almaty Districts and avg pollutant matter for corresponding districts\n",
    "\n",
    "        Args:\n",
    "            gdf (gpd.GeoDataFrame): GeoDataFrame with sensor readings, locations etc. (usually after filtering, for specific dates)\n",
    "\n",
    "        Returns:\n",
    "            gpd.GeoDataFrame: gdf that carries district names, geometries, and avg_pm25\n",
    "        \"\"\"\n",
    "        # sjoin stands for spatial join in geopandas\n",
    "        # how='inner' retains the rows where Point of sensor loc satisfy the boundaries of districts\n",
    "        # I use inner join since some sensors may be located outside of any districts\n",
    "        # predicate='intersets' returns True if and only if the Point(Sensor) instersects with interior OR boundary of district\n",
    "        # To get more information about spatial joins: https://geopandas.org/en/stable/docs/user_guide/mergingdata.html\n",
    "        # To get more information predicates: https://shapely.readthedocs.io/en/latest/manual.html#binary-predicates\n",
    "        joined_gdf = gpd.sjoin(gdf, self.almaty_boundaries, how='inner', predicate='intersects')\n",
    "\n",
    "        # name_right would be the district name (since it is the right argument in the sjoin)\n",
    "        # from the DataFrameGroupBy, we are interested in the mean of Readings\n",
    "        # this results in district name in one column, and avg. sensor reading in another column of district_pollution\n",
    "        district_pollution = joined_gdf.groupby('name_right')['Reading'].mean().reset_index()\n",
    "\n",
    "        # for the further purposes of the project, I incorporate the 'avg_pm25' into boundaries GeoDataFrame (please, revisit this code) !!!!!!!\n",
    "        almaty_boundaries = self.almaty_boundaries.copy()\n",
    "        almaty_boundaries = almaty_boundaries.set_index('name')\n",
    "        district_pollution = district_pollution.set_index('name_right')\n",
    "        almaty_boundaries = almaty_boundaries.join(district_pollution)\n",
    "\n",
    "        almaty_boundaries.rename(columns={'Reading': 'avg_pm25'}, inplace=True)\n",
    "\n",
    "        # without resetting the index, dataframe takes really weird form. I wanted to escape that\n",
    "        almaty_boundaries.reset_index(inplace=True)\n",
    "        \n",
    "        return almaty_boundaries\n",
    "    \n",
    "\n",
    "    def interpolate(self, gdf: gpd.GeoDataFrame, method: str = 'No Interpolation', grid_spacing: float = 0.005):\n",
    "        \"\"\"\n",
    "        Does interpolation necessary for plotting purposes\n",
    "\n",
    "        Args:\n",
    "            gdf (gpd.GeoDataFrame): gdf with sensor readings, locations etc.\n",
    "            method (str, optional): Chosen interpolation method. Defaults to 'No Interpolation'.\n",
    "            grid_spacing (float, optional): Spacing for interpolation method. Defaults to 0.005.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Masked interpolated grid, longitude grid, and latitude grid\n",
    "        \"\"\"\n",
    "        \n",
    "        sensor_points = gdf[['geometry', 'Reading']].dropna(subset=['Reading'])\n",
    "\n",
    "        if sensor_points.empty:\n",
    "            tk.messagebox.showwarning(\"No Data\", \"No sensor data available for the selected date.\")\n",
    "            return\n",
    "\n",
    "        lons = sensor_points.geometry.x.values\n",
    "        lats = sensor_points.geometry.y.values\n",
    "        readings = sensor_points['Reading'].values\n",
    "\n",
    "        min_lon, min_lat, max_lon, max_lat = almaty_boundaries.total_bounds\n",
    "        grid_spacing = 0.005  # Adjust this for desired resolution\n",
    "        grid_lon = np.arange(min_lon, max_lon, grid_spacing)\n",
    "        grid_lat = np.arange(min_lat, max_lat, grid_spacing)\n",
    "        grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n",
    "\n",
    "        # Choose interpolation method\n",
    "        if method == 'Nearest neighbor':\n",
    "            # Nearest neighbor interpolation using griddata\n",
    "            grid_z = griddata(\n",
    "                points=(lons, lats),\n",
    "                values=readings,\n",
    "                xi=(grid_lon, grid_lat),\n",
    "                method='nearest'\n",
    "            )\n",
    "        elif method == 'RBF':\n",
    "            # RBF interpolation\n",
    "            rbf_interpolator = Rbf(lons, lats, readings, function='multiquadric', smooth=0)\n",
    "            grid_z = rbf_interpolator(grid_lon, grid_lat)\n",
    "            grid_z = np.clip(grid_z, 0, 2)\n",
    "        else:\n",
    "            # Show the data without any interpolation\n",
    "            grid_z = np.full(grid_lon.shape, np.nan)\n",
    "\n",
    "        # Mask out areas outside Almaty boundaries\n",
    "        boundary_polygon = almaty_boundaries.unary_union\n",
    "\n",
    "        # Create grid points as a GeoDataFrame\n",
    "        grid_points = np.vstack((grid_lon.flatten(), grid_lat.flatten())).T\n",
    "        grid_points_gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(grid_points[:,0], grid_points[:,1]), crs=almaty_boundaries.crs)\n",
    "\n",
    "        # Check which points are within the boundary polygon\n",
    "        grid_points_gdf['inside'] = grid_points_gdf.within(boundary_polygon)\n",
    "\n",
    "        # Create mask (True for points outside the boundary)\n",
    "        mask = ~grid_points_gdf['inside'].values.reshape(grid_lon.shape)\n",
    "\n",
    "        # Apply mask to the interpolated grid\n",
    "        grid_z_masked = np.ma.array(grid_z, mask=mask)\n",
    "\n",
    "        return grid_z_masked, grid_lon, grid_lat\n",
    "\n",
    "almaty_boundaries = gpd.read_file('notebook/almaty-districts.geo.json')\n",
    "dataManipulator = DataManipulator(combined_gdf, almaty_boundaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Discuss how you chose what to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_phys4038",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
